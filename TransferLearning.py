# -*- coding: utf-8 -*-
"""pretrainedModel(0_3655732) (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14_8uOsDHkKKAKmBAQ1vNk464yY61KMFs
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt


from pandas import DataFrame

# import warnings
import warnings
# filter warnings
warnings.filterwarnings('ignore')



# Any results you write to the current directory are saved as output.

# read train 
train = pd.read_csv("drive/MyDrive/MLdata/train.csv")
print(train.shape)
train.head()

# put labels into y_train variable
Y_train = train["Y"]
# Drop 'Y' column
X_train = train.drop(labels = ["Y"],axis = 1)

from google.colab import drive
drive.mount('/content/drive')

x1=X_train.iloc[:1000,0:463]
x2=X_train.iloc[:1000,463:926]
x3=X_train.iloc[:1000,926:1389]
x4=X_train.iloc[:1000,1389:1852]
x5=X_train.iloc[:1000,1852:2315]
x6=X_train.iloc[:1000,2315:2778]
x7=X_train.iloc[:1000,2778:3241]
print(x1)
print(x2)
x8=X_train.iloc[:1000,3241:3704]
x9=X_train.iloc[:1000,3704:4167]
x10=X_train.iloc[:1000,4167:4630]
x11=X_train.iloc[:1000,4630:5093]
x12=X_train.iloc[:1000,5093:5556]
x13=X_train.iloc[:1000,5556:6019]
x14=X_train.iloc[:1000,6019:6482]
x15=X_train.iloc[:1000,6482:6945]



x1=x1.to_numpy()
x2=x2.to_numpy()
x3=x3.to_numpy()
x4=x4.to_numpy()
x5=x5.to_numpy()
x6=x6.to_numpy()
x7=x7.to_numpy()
x8=x8.to_numpy()
x9=x9.to_numpy()
x10=x10.to_numpy()
x11=x11.to_numpy()
x12=x12.to_numpy()
x13=x13.to_numpy()
x14=x14.to_numpy()
x15=x15.to_numpy()

y_train=Y_train.to_numpy()

print(x1.shape)



# read test 
test= pd.read_csv("drive/MyDrive/MLdata/test.csv")
#print(test.shape)
test.head()

# put labels into y_test variable
Y_test = test["Y"]
# Drop 'Y' column
X_test = test.drop(labels = ["Y"],axis = 1)

t1=X_test.iloc[:100,0:463]
t2=X_test.iloc[:100,463:926]
t3=X_test.iloc[:100,926:1389]
t4=X_test.iloc[:100,1389:1852]
t5=X_test.iloc[:100,1852:2315]
t6=X_test.iloc[:100,2315:2778]
t7=X_test.iloc[:100,2778:3241]
t8=X_test.iloc[:100,3241:3704]
t9=X_test.iloc[:100,3704:4167]
t10=X_test.iloc[:100,4167:4630]
t11=X_test.iloc[:100,4630:5093]
t12=X_test.iloc[:100,5093:5556]
t13=X_test.iloc[:100,5556:6019]
t14=X_test.iloc[:100,6019:6482]
t15=X_test.iloc[:100,6482:6945]
t1=t1.to_numpy()
t2=t2.to_numpy()
t3=t3.to_numpy()
t4=t4.to_numpy()
t5=t5.to_numpy()
t6=t6.to_numpy()
t7=t7.to_numpy()
t8=t8.to_numpy()
t9=t9.to_numpy()
t10=t10.to_numpy()
t11=t11.to_numpy()
t12=t12.to_numpy()
t13=t13.to_numpy()
t14=t14.to_numpy()
t15=t15.to_numpy()


 
y_test=Y_test.to_numpy() 


import numpy as np
from sklearn.preprocessing import normalize


avg = np.average(y_train)
std = np.std(y_train)


normalized_y_train = (y_train-avg)/std
print(normalized_y_train)


normalized_y_test = (y_test-avg)/std
print(normalized_y_test)

#data

#reshape


x1=x1.reshape(1000,1,463)
x2=x2.reshape(1000,1,463)
x3=x3.reshape(1000,1,463)
x4=x4.reshape(1000,1,463)
x5=x5.reshape(1000,1,463)
x6=x6.reshape(1000,1,463)
x7=x7.reshape(1000,1,463)
x8=x8.reshape(1000,1,463)
x9=x9.reshape(1000,1,463)
x10=x10.reshape(1000,1,463)
x11=x11.reshape(1000,1,463)
x12=x12.reshape(1000,1,463)
x13=x13.reshape(1000,1,463)
x14=x14.reshape(1000,1,463)
x15=x15.reshape(1000,1,463)
t1=t1.reshape(100,1,463)
t2=t2.reshape(100,1,463)
t3=t3.reshape(100,1,463)
t4=t4.reshape(100,1,463)
t5=t5.reshape(100,1,463)
t6=t6.reshape(100,1,463)
t7=t7.reshape(100,1,463)
t8=t8.reshape(100,1,463)
t9=t9.reshape(100,1,463)
t10=t10.reshape(100,1,463)
t11=t11.reshape(100,1,463)
t12=t12.reshape(100,1,463)
t13=t13.reshape(100,1,463)
t14=t14.reshape(100,1,463)
t15=t15.reshape(100,1,463)

#model
# Multiple Inputs Model
from keras.utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import Flatten
from tensorflow.keras import layers
from keras.layers.convolutional import Conv2D
from keras.layers.pooling import MaxPooling2D
from keras.layers.merge import concatenate

import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam, RMSprop
import numpy as np

inputshape=(1,463)

input1 = Input(shape=inputshape)
input2 = Input(shape=inputshape)
input3 = Input(shape=inputshape)
input4 = Input(shape=inputshape)
input5 = Input(shape=inputshape)
input6 = Input(shape=inputshape)
input7 = Input(shape=inputshape)
input8 = Input(shape=(inputshape))
input9 = Input(shape=(inputshape))
input10 = Input(shape=(inputshape))
input11 = Input(shape=(inputshape))
input12 = Input(shape=(inputshape))
input13 = Input(shape=(inputshape))
input14 = Input(shape=(inputshape))
input15 = Input(shape=(inputshape))



conv1 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input1)
conv2 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input2)
conv3 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input3)
conv4 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input4)
conv5 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input5)
conv6 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input6)
conv7 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input7)
conv8 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input8)
conv9 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input9)
conv10 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input10)
conv11 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input11)
conv12 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input12)
conv13 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input13)
conv14 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input14)
conv15 = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(input15)

pool1 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv1)
pool2 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv2)
pool3 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv3)
pool4 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv4)
pool5 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv5)
pool6 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv6)
pool7 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv7)
pool8 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv8)
pool9 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv9)
pool10 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv10)
pool11 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv11)
pool12 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv12)
pool13 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv13)
pool14 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv14)
pool15 = MaxPooling1D(pool_size=2, strides=1, padding='same')(conv15)

flat1 = Flatten(name='flat1')(pool1)
flat2 = Flatten(name='flat2')(pool2)
flat3 = Flatten(name='flat3')(pool3)
flat4 = Flatten(name='flat4')(pool4)
flat5 = Flatten(name='flat5')(pool5)
flat6 = Flatten(name='flat6')(pool6)
flat7 = Flatten(name='flat7')(pool7)
flat8 = Flatten(name='flat8')(pool8)
flat9 = Flatten(name='flat9')(pool9)
flat10 = Flatten(name='flat10')(pool10)
flat11 = Flatten(name='flat11')(pool11)
flat12 = Flatten(name='flat12')(pool12)
flat13 = Flatten(name='flat13')(pool13)
flat14 = Flatten(name='flat14')(pool14)
flat15 = Flatten(name='flat15')(pool15)



drop1 = Dropout(0.8)(flat1)
drop2 = Dropout(0.8)(flat2)
drop3 = Dropout(0.8)(flat3)
drop4 = Dropout(0.8)(flat4)
drop5 = Dropout(0.8)(flat5)
drop6 = Dropout(0.8)(flat6)
drop7 = Dropout(0.8)(flat7)
drop8 = Dropout(0.9)(flat8)
drop9 = Dropout(0.8)(flat9)
drop10 = Dropout(0.8)(flat10)
drop11 = Dropout(0.7)(flat11)
drop12 = Dropout(0.9)(flat12)
drop13 = Dropout(0.8)(flat13)
drop14 = Dropout(0.8)(flat14)
drop15 = Dropout(0.6)(flat15)


c1 = Dense(2, activation='relu')(drop1)
c2 = Dense(2, activation='tanh')(drop2)
c3 = Dense(2, activation='relu')(drop3)
c4 = Dense(2, activation='relu')(drop4)
c5 = Dense(2, activation='tanh')(drop5)
c6 = Dense(2, activation='relu')(drop6)
c7 = Dense(2, activation='relu')(drop7)
c8 = Dense(2, activation='relu')(drop8)
c9 = Dense(2, activation='tanh')(drop9)
c10 = Dense(2, activation='relu')(drop10)
c11 = Dense(2, activation='relu')(drop11)
c12 = Dense(2, activation='relu')(drop12)
c13 = Dense(2, activation='relu')(drop13)
c14 = Dense(2, activation='relu')(drop14)
c15 = Dense(2, activation='relu')(drop15)




input = Concatenate()([c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15])

#batch normolization
bn1 = BatchNormalization(momentum=0.9)(input)

x = Dense(1, activation='tanh')(bn1)
model = Model(inputs=[input1, input2,input3,input4,input5,input6,input7,input8,input9,input10,input11,input12,input13,input14,input15], outputs=x)
model.summary()
plot_model(model, to_file='multiple_inputs.png')

#model compile
"""
model.compile(
    optimizer = RMSprop(lr=0.02,rho=0.9,epsilon=None,decay=0),
    loss = 'mse'
)  
"""  
# 編譯: 選擇損失函數、優化方法及成效衡量方式

model.compile(loss='mse', optimizer =Adam(lr=0.005), metrics=['mae']) #learning rate

# checkpoint
from keras.callbacks import ModelCheckpoint
filepath="weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=1, save_best_only=True,mode='min')
callbacks_list = [checkpoint]

# serialize model to JSON
model_json = model.to_json()
with open("pre_trained.json", "w") as json_file:
    json_file.write(model_json)

from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_mae', mode='min', verbose=1,patience=500)
#model fit
x_test=[t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,t11,t12,t13,t14,t15]
train_history = model.fit(x=[x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15],  
              y=normalized_y_train,   
              epochs=30000, batch_size=128, verbose=2,validation_data=(x_test, normalized_y_test),callbacks=[early_stopping,callbacks_list],) #epochs Y標準化 畫圖時Reverse hdf5

#load model
from keras.models import load_model
 

# 載入模型
model = load_model('weights.best.hdf5')

results = model.evaluate(x=[t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,t11,t12,t13,t14,t15], y=normalized_y_test, batch_size=64)
print("test loss, test acc:", results)

# The predict() method - predict the outputs for the given inputs 
predict=model.predict([t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,t11,t12,t13,t14,t15])


predict=predict*std+avg


plt.plot(y_test, 'x', color='red');
plt.plot(predict)
plt.title('prediction diagram')
plt.ylabel('Y')
plt.xlabel('data')
plt.legend(['test', 'predict'], loc='upper left')
plt.show()
sum=0
for i in range(100):
  sum=sum+abs(predict[i]-y_test[i])
sum=sum/100
print("MAE:",sum)

"""
# summarize history for accuracy
plt.plot(train_history.history['mae'])
plt.plot(train_history.history['val_mae'])


plt.title('mae')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(train_history.history['loss'])
plt.plot(train_history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
"""

#load Target data

TRENCH = pd.read_csv("drive/MyDrive/MLdata/TRENCH_1100_new.csv")
#TRENCH.head()

Y = TRENCH["Y"]
trainY = Y.iloc[:300]
#print(trainY)
testY = Y.iloc[1000:1100]
#print(testY)
trainX = TRENCH.drop(labels = ["Y"],axis = 1)
#trainX.head() 


x1=trainX.iloc[:300,0:463]
x2=trainX.iloc[:300,463:926]
x3=trainX.iloc[:300,926:1389]
x4=trainX.iloc[:300,1389:1852]
x5=trainX.iloc[:300,1852:2315]
x6=trainX.iloc[:300,2315:2778]
x7=trainX.iloc[:300,2778:3241]
#print(x1)
#print(x2)
x8=trainX.iloc[:300,3241:3704]
x9=trainX.iloc[:300,3704:4167]
x10=trainX.iloc[:300,4167:4630]
x11=trainX.iloc[:300,4630:5093]
x12=trainX.iloc[:300,5093:5556]
x13=trainX.iloc[:300,5556:6019]
x14=trainX.iloc[:300,6019:6482]
x15=trainX.iloc[:300,6482:6945]

t1=trainX.iloc[1000:1100,0:463]
t2=trainX.iloc[1000:1100,463:926]
t3=trainX.iloc[1000:1100,926:1389]
t4=trainX.iloc[1000:1100,1389:1852]
t5=trainX.iloc[1000:1100,1852:2315]
t6=trainX.iloc[1000:1100,2315:2778]
t7=trainX.iloc[1000:1100,2778:3241]
#print(t1)
#print(t2)
t8=trainX.iloc[1000:1100,3241:3704]
t9=trainX.iloc[1000:1100,3704:4167]
t10=trainX.iloc[1000:1100,4167:4630]
t11=trainX.iloc[1000:1100,4630:5093]
t12=trainX.iloc[1000:1100,5093:5556]
t13=trainX.iloc[1000:1100,5556:6019]
t14=trainX.iloc[1000:1100,6019:6482]
t15=trainX.iloc[1000:1100,6482:6945]

x1=x1.to_numpy()
x2=x2.to_numpy()
x3=x3.to_numpy()
x4=x4.to_numpy()
x5=x5.to_numpy()
x6=x6.to_numpy()
x7=x7.to_numpy()
x8=x8.to_numpy()
x9=x9.to_numpy()
x10=x10.to_numpy()
x11=x11.to_numpy()
x12=x12.to_numpy()
x13=x13.to_numpy()
x14=x14.to_numpy()
x15=x15.to_numpy()
t1=t1.to_numpy()
t2=t2.to_numpy()
t3=t3.to_numpy()
t4=t4.to_numpy()
t5=t5.to_numpy()
t6=t6.to_numpy()
t7=t7.to_numpy()
t8=t8.to_numpy()
t9=t9.to_numpy()
t10=t10.to_numpy()
t11=t11.to_numpy()
t12=t12.to_numpy()
t13=t13.to_numpy()
t14=t14.to_numpy()
t15=t15.to_numpy()
trainY=trainY.to_numpy()
testY=testY.to_numpy()



x1=x1.reshape(300,1,463)
x2=x2.reshape(300,1,463)
x3=x3.reshape(300,1,463)
x4=x4.reshape(300,1,463)
x5=x5.reshape(300,1,463)
x6=x6.reshape(300,1,463)
x7=x7.reshape(300,1,463)
x8=x8.reshape(300,1,463)
x9=x9.reshape(300,1,463)
x10=x10.reshape(300,1,463)
x11=x11.reshape(300,1,463)
x12=x12.reshape(300,1,463)
x13=x13.reshape(300,1,463)
x14=x14.reshape(300,1,463)
x15=x15.reshape(300,1,463)
t1=t1.reshape(100,1,463)
t2=t2.reshape(100,1,463)
t3=t3.reshape(100,1,463)
t4=t4.reshape(100,1,463)
t5=t5.reshape(100,1,463)
t6=t6.reshape(100,1,463)
t7=t7.reshape(100,1,463)
t8=t8.reshape(100,1,463)
t9=t9.reshape(100,1,463)
t10=t10.reshape(100,1,463)
t11=t11.reshape(100,1,463)
t12=t12.reshape(100,1,463)
t13=t13.reshape(100,1,463)
t14=t14.reshape(100,1,463)
t15=t15.reshape(100,1,463)


avg = np.average(trainY)
std = np.std(trainY)

normalized_y_train = (trainY-avg)/std
#print(normalized_y_train)

normalized_y_test = (testY-avg)/std
#print(normalized_y_test)



#S5 = pd.read_csv("drive/MyDrive/MLdata/PM3_S5.csv")
#S5.head()

#S7 = pd.read_csv("drive/MyDrive/MLdata/PM3_S7.csv")
#S7.head()

#New Model
import keras
with open('pre_trained.json','r') as f:
  model_json = f.read()

#inputs = keras.Input(shape=np.array([input1, input2,input3,input4,input5,input6,input7,input8,input9,input10,input11,input12,input13,input14,input15]).shape)

#newModel = tf.keras.models.model_from_json(model_json)
#newModel.load_weights('weights.best.hdf5')
# 載入模型

model = load_model('weights.best.hdf5')
#model.summary()
newModel=Sequential()

"""
for layer in model.layers[0:14]:
  newModel.add(layer)
"""
"""
for layer in model.layers[15:29]:
  last_layer = layer.output
  conv = layers.Conv1D( filters=6,kernel_size=3, input_shape=inputshape, padding='same',activation='tanh')(last_layer)
"""  
ll = model.layers[90].output


ll = Dense(1)(ll)

new_model = Model(inputs=model.input,outputs=ll)


    
  


print(new_model.summary())


# 編譯: 選擇損失函數、優化方法及成效衡量方式

new_model.compile(loss='mse', optimizer =Adam(lr=0.005), metrics=['mae']) #learning rate

early_stopping = EarlyStopping(monitor='val_mae', mode='min', verbose=1,patience=500)

filepath="transfer.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=1, save_best_only=True,mode='min')
callbacks_list = [checkpoint]

#model fit
x_test=[t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,t11,t12,t13,t14,t15]
train_history = new_model.fit(x=[x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15],  
              y=normalized_y_train, 
              epochs=30000, batch_size=128, verbose=2,validation_data=(x_test, normalized_y_test),callbacks=[early_stopping,callbacks_list],) #epochs Y標準化 畫圖時Reverse hdf5

# 載入模型
model = load_model('transfer.best.hdf5')

results = model.evaluate(x=[t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,t11,t12,t13,t14,t15], y=normalized_y_test, batch_size=64)
print("test loss, test acc:", results)

# The predict() method - predict the outputs for the given inputs 
predict=model.predict([t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,t11,t12,t13,t14,t15])
#plt.axis([0,100,80,90]) 

predict=predict*std+avg


#print(predict)
#plt.plot(predict,Y_test)
#plt.plot(predict, 'o', color='black');
plt.plot(testY, 'x', color='red');
plt.plot(predict)
plt.title('prediction diagram')
plt.ylabel('Y')
plt.xlabel('data')
plt.legend(['test', 'predict'], loc='upper left')
plt.show()
sum=0
for i in range(100):
  sum=sum+abs(predict[i]-testY[i])
sum=sum/100
print("MAE:",sum)